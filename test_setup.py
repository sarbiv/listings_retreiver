#!/usr/bin/env python3
"""
Test script to verify the luxury development scraper setup
"""
import sys
import os
from pathlib import Path

def test_imports():
    """Test that all modules can be imported"""
    print("Testing imports...")
    
    try:
        import scrapy
        print("‚úì Scrapy imported successfully")
    except ImportError as e:
        print(f"‚úó Scrapy import failed: {e}")
        return False
    
    try:
        import pydantic
        print("‚úì Pydantic imported successfully")
    except ImportError as e:
        print(f"‚úó Pydantic import failed: {e}")
        return False
    
    try:
        import pandas
        print("‚úì Pandas imported successfully")
    except ImportError as e:
        print(f"‚úó Pandas import failed: {e}")
        return False
    
    try:
        import sqlalchemy
        print("‚úì SQLAlchemy imported successfully")
    except ImportError as e:
        print(f"‚úó SQLAlchemy import failed: {e}")
        return False
    
    try:
        import structlog
        print("‚úì Structlog imported successfully")
    except ImportError as e:
        print(f"‚úó Structlog import failed: {e}")
        return False
    
    try:
        import yaml
        print("‚úì PyYAML imported successfully")
    except ImportError as e:
        print(f"‚úó PyYAML import failed: {e}")
        return False
    
    return True

def test_scraper_modules():
    """Test that scraper modules can be imported"""
    print("\nTesting scraper modules...")
    
    try:
        from scraper.db.models import Project, Unit, Amenity, MediaLink, Source
        print("‚úì Database models imported successfully")
    except ImportError as e:
        print(f"‚úó Database models import failed: {e}")
        return False
    
    try:
        from scraper.schemas import ProjectIn, UnitIn, AmenityIn
        print("‚úì Pydantic schemas imported successfully")
    except ImportError as e:
        print(f"‚úó Pydantic schemas import failed: {e}")
        return False
    
    try:
        from scraper.pipelines.clean_normalize import CleanNormalizePipeline
        print("‚úì Cleaning pipeline imported successfully")
    except ImportError as e:
        print(f"‚úó Cleaning pipeline import failed: {e}")
        return False
    
    try:
        from scraper.pipelines.dedupe import DedupePipeline
        print("‚úì Deduplication pipeline imported successfully")
    except ImportError as e:
        print(f"‚úó Deduplication pipeline import failed: {e}")
        return False
    
    try:
        from scraper.pipelines.database import DatabasePipeline
        print("‚úì Database pipeline imported successfully")
    except ImportError as e:
        print(f"‚úó Database pipeline import failed: {e}")
        return False
    
    try:
        from scraper.spiders.tier_a.selene_fort_lauderdale import SeleneFortLauderdaleSpider
        print("‚úì Tier A spider imported successfully")
    except ImportError as e:
        print(f"‚úó Tier A spider import failed: {e}")
        return False
    
    try:
        from scraper.spiders.tier_b.propertyguru_sg import PropertyGuruSgSpider
        print("‚úì Tier B spider imported successfully")
    except ImportError as e:
        print(f"‚úó Tier B spider import failed: {e}")
        return False
    
    return True

def test_database_creation():
    """Test database creation"""
    print("\nTesting database creation...")
    
    try:
        from scraper.db.io import DatabaseManager
        
        # Create temporary database
        db = DatabaseManager("sqlite:///:memory:")
        db.init_database()
        print("‚úì Database created successfully")
        
        # Test basic operations
        stats = db.get_stats()
        print(f"‚úì Database stats retrieved: {stats}")
        
        return True
    except Exception as e:
        print(f"‚úó Database creation failed: {e}")
        return False

def test_config_loading():
    """Test configuration loading"""
    print("\nTesting configuration loading...")
    
    try:
        import yaml
        config_path = Path(__file__).parent / 'scraper' / 'config.yaml'
        
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            print("‚úì Configuration loaded successfully")
            print(f"  - Default download delay: {config['defaults']['download_delay_sec']}s")
            print(f"  - Sites configured: {len(config['sites'])}")
            return True
        else:
            print("‚úó Configuration file not found")
            return False
    except Exception as e:
        print(f"‚úó Configuration loading failed: {e}")
        return False

def main():
    """Run all tests"""
    print("üè¢ Luxury Development Scraper - Setup Test")
    print("=" * 50)
    
    tests = [
        test_imports,
        test_scraper_modules,
        test_database_creation,
        test_config_loading
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
        print()
    
    print("=" * 50)
    print(f"Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All tests passed! The scraper is ready to use.")
        print("\nNext steps:")
        print("1. Initialize database: python -m scraper.cli init-db")
        print("2. Run a spider: python -m scraper.cli crawl --spider selene_fort_lauderdale")
        print("3. Export data: python -m scraper.cli export --table projects --output data/exports/projects.csv")
        return 0
    else:
        print("‚ùå Some tests failed. Please check the errors above.")
        return 1

if __name__ == '__main__':
    sys.exit(main())
